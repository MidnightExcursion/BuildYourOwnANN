{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of a \"Create-Your-Own-ANN\" Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1): Introduction:\n",
    "\n",
    "## (1.1) Introduction about ANNs:\n",
    "\n",
    "In the following notebook, we will attempt to understand the idea of *Artificial Neural Networks (ANN) as being a Universal Function Approximator*.\n",
    "\n",
    "While the words are fancy, the idea is simple: Artificial Neural Networks are fundamentally mathematical models that attempt to \"learn\" an unknown, \"underlying\" function. The learning process is guided by the tuning of several *parameters* in the network (that come in the form of weights and biases) based on the purely mathematical idea of optimizing a *loss function*. Let's briefly discuss the notation that we'll use to approach the problem:\n",
    "\n",
    "Suppose some physical law relates quantities $x$ and $Y$ in a linear way: $y = f(x) = \\alpha x$. Here $\\alpha$ is a constant number that we call a \"parameter.\" In our example, we may assume it to be \"fundamental constant of Nature.\" Then, what we have is an \"underlying function\" that is $f(x) = \\alpha x$. An experiment may measure $y$ several times with knowledge of $x$. We take it as a given that the experiment is, in some sense, by definition will not return for us a perfect value of $y$. In other words, while we might know $x$ to within  $\\sigma_{x} = 5\\%$, we might only be able to measure $y$ to within $\\sigma_{y} = 10\\%$. ($\\sigma_{x}$ is to be read as \"uncertainty in $x$.\") So, the main challenge can now be worded as follows: *Supposing we don't know the underlying function of nature but can measure $x$ and $y$. Can, given enough (experimental) data, can an ANN by itself learn the underlying function even with the uncertainty in the data?*\n",
    "\n",
    "## (1.2): Introduction about the Function:\n",
    "\n",
    "We're not only concerned about the creation of a standard, feed-forward, densely-connected ANN, but we're also going to design a user-controlled workflow that sets up an entire TensorFlow model just with user input. Instead of hard-coding seventeen models in fifteen hours, we're going to code a function that essentially allows the user to define whatever TensorFlow model they want on the spot. If we can truly realize this idea, then we'll try to \"generalize\" the function to allow for more customization of loss functions and activation functions and so on. Right now, what we want is a proof of concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2): Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tensorflow\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll obviously need Numpy. We'll obviously need TensorFlow. Later, we'll do some plotting, which makes the import of Matplotlib necessary, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2): Define Unusual, Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTEGER_ZERO = 0\n",
    "\n",
    "PANDAS_AXIS_ROWS = 0 # Just a setting\n",
    "PANDAS_AXIS_COLUMNS = 1 # Just a setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I always forget what the stupid documentation requires certain keyword arguments to be. So, isntead of me forgetting every single time, I am just going to make stupid global (static) variables that remind me of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3): Wrapper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know what to say. We need to time (and describe) these things so that we know exactly what is happening and how long it took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_output_desciption_wrapper(function_to_describe, verbose = False):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    A wrapper function that will explicitly describe what the output of a \n",
    "    function with name NAME actually was and its type.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    function_to_describe : (func)\n",
    "\n",
    "    verbose : (bool)\n",
    "\n",
    "    \n",
    "    Function Flow\n",
    "    --------------\n",
    "    (1): All we do is obtain the function name and say that it's currently running.\n",
    "    (2): Next, we execute the function, and obtain its output.\n",
    "    (3): We also obtain the datatype of the output.\n",
    "    (4): We remark that the function has finished running.\n",
    "    (5): Then, we just print the output of the function and its type.\n",
    "\n",
    "    \n",
    "    Notes\n",
    "    --------------\n",
    "    None\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"> Function \\\"{function_to_describe.__name__}\\\" is now running...\")\n",
    "\n",
    "        output_of_function = function_to_describe(*args, **kwargs)\n",
    "        type_of_function_output = type(output_of_function)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"> Function \\\"{function_to_describe.__name__}\\\" finished running. \")\n",
    "\n",
    "        print(f\"> The output of \\\"{function_to_describe.__name__}\\\" was:\\n> {output_of_function} ({type_of_function_output})\")\n",
    "        \n",
    "        return output_of_function\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "def timer_wrapper(function_to_time, verbose = False):\n",
    "    \"\"\"\n",
    "    This is a wrapper function that serves as a decorator\n",
    "    for other functions that informs us how long (in seconds)\n",
    "    it takes for those functions to run.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    function_to_time : (func)\n",
    "    \n",
    "    verbose : (bool)\n",
    "\n",
    "\n",
    "    ----- FUNCTION FLOW -----\n",
    "    (1): Try to import the native module \"Time.\"\n",
    "    (2): Start the timer.\n",
    "    (3): Run the function.\n",
    "    (4): End the timer when the variable \"function_result\" has\n",
    "        been obtained.\n",
    "    (5): Compute the difference in end time from start time.\n",
    "    (6): Log the output, which states how long the function took \n",
    "        to run.\n",
    "    -------------------------\n",
    "\n",
    "    \n",
    "    Notes\n",
    "    --------------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # (1): Try to import the native module \"Time\":\n",
    "    try:\n",
    "        import time\n",
    "    except Exception as E:\n",
    "        print(f\"> Error running timer wrapper:\\n> {E}\")\n",
    "        return 0\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "\n",
    "        # (2): Start the timer.\n",
    "        start_time = time.time()\n",
    "        if verbose:\n",
    "            print(f\"> Function \\\"{function_to_time.__name__}\\\" began running {start_time}.\")\n",
    "\n",
    "        # (3): Run the function.\n",
    "        function_result = function_to_time(*args, **kwargs)\n",
    "    \n",
    "        # (4): End the timer.\n",
    "        end_time = time.time()\n",
    "        if verbose:\n",
    "            print(f\"> Function \\\"{function_to_time.__name__}\\\" began running {start_time}.\")\n",
    "\n",
    "        # (5): Compute the difference.\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # (6): Log the output.\n",
    "        print(f\"> Function \\\"{function_to_time.__name__}\\\" took {elapsed_time:.6f} seconds to run.\")\n",
    "        \n",
    "        return function_result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4): Designing the \"Experimental Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to construct the major functions that we're going to use. Recall (the trivial expression) that we want to assume some \"fundamental law of nature\" has the form $y(x) = A x^{2} + B x + C$. We are going to attempt to guess this fundamental law of nature, which clearly involves the extraction of the three parameters $A, B,$ and $C$. In this unusual sense, nature has \"encoded\" three fundamental constants that we need to attempt to derive using neural network techniques.\n",
    "\n",
    "The idea is the following:\n",
    "\n",
    "(i): First, we'll make a standard quadratic function that takes a given $x$ and those three parameters. This function is obviously essential when we assess the veracity of the model.\n",
    "\n",
    "(ii): In order to simulate experimental \"uncertainty,\" we'll code a function that allows sampling from a Normal Distribution. Remember that a Normal Distribution ($\\mathcal{N} (\\mu, \\sigma)$) is characterized by the mean ($\\mu$) and the standard deviation ($\\sigma$), and that sampling from this function means that you'll have a probability of $P \\approx 68.2 \\%$ of obtaining values of $\\mu \\pm \\sigma$ and $P \\approx 95\\%$ of obtaining values of $\\mu \\pm 2 \\sigma$.\n",
    "\n",
    "(iii): We'll also need to then write a function that also samples our underlying function -- the quadratic. Our experiment is designed to measure $y$ to within some uncertainty. That uncertainty comes from the sampling as well. Instead of plugging-and-chugging into the quadratic equation for some given value of $x$ and calling it a day, we first plug-and-chug to obtain the accurate value of $y$, and then we'll use that value of $y$ as the mean in another Normal Distribution characterized by $\\mathcal{N}(\\mu = y, \\sigma_{y} = ?)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.1): Quadratic Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer_wrapper\n",
    "def quadratic_function(x, coefficient_a, coefficient_b, coefficient_c):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Returns the output of a quadratic function given an input and the \n",
    "    standard three parameters characterizing a quadratic function.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    x : (num)\n",
    "\n",
    "    coefficient_a : (num)\n",
    "\n",
    "    coefficient_b : (num)\n",
    "\n",
    "    coefficient_c : (num)\n",
    "\n",
    "\n",
    "    Function Flow\n",
    "    --------------\n",
    "\n",
    "    \n",
    "    Notes\n",
    "    --------------\n",
    "    (1): The quadratic function has three \"parameters.\" These are roughly\n",
    "        called \"A\", \"B\", and \"C.\" Only after specifying those three parameters\n",
    "        will the function actually return an output given an input. In other\n",
    "        words, besides supplying an independent variable, you need those\n",
    "        three other pieces of information -- the parameters -- in order to\n",
    "        get a number as an ouput.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        corresponding_y_value = coefficient_a * x**2 + coefficient_b * x + coefficient_c\n",
    "        return corresponding_y_value\n",
    "    except Exception as E:\n",
    "        return INTEGER_ZERO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.2): Sampling a Normal Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer_wrapper\n",
    "def sample_normal_distribution(mean_value, standard_deviation):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Uses Numpy's function random.normal() to sample a normal distribution\n",
    "    with a mean of \"loc\" and a standard deviation of \"scale,\" the two WORST\n",
    "    KWARGS I'VE EVER SEEN IN PYTHON.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    mean : (float)\n",
    "\n",
    "    standard_deviation : (float)\n",
    "\n",
    "\n",
    "    Function Flow\n",
    "    --------------\n",
    "\n",
    "    \n",
    "    Notes\n",
    "    --------------\n",
    "    (1): The quadratic function has three \"parameters.\" These are roughly\n",
    "        called \"A\", \"B\", and \"C.\" Only after specifying those three parameters\n",
    "        will the function actually return an output given an input. In other\n",
    "        words, besides supplying an independent variable, you need those\n",
    "        three other pieces of information -- the parameters -- in order to\n",
    "        get a number as an ouput.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        randomly_sampled_variable = np.random.normal(\n",
    "            loc = mean_value,\n",
    "            scale = standard_deviation\n",
    "        )\n",
    "        return randomly_sampled_variable\n",
    "    \n",
    "    except Exception as E:\n",
    "        print(f\"> Error in sampling normal distribution:\\n> {E}!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.3): Sample Underlying Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer_wrapper\n",
    "def sample_underlying_function(x_value):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    This function provides a method to generate \n",
    "    so-called \"experimental data\" that comes from an\n",
    "    underlying function.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "\n",
    "    Function Flow\n",
    "    --------------\n",
    "\n",
    "    Notes\n",
    "    --------------\n",
    "    None\n",
    "    \"\"\"\n",
    "    _PARAMETER_A = 2\n",
    "    _PARAMETER_B = -3\n",
    "    _PARAMETER_C = 1\n",
    "\n",
    "    _RANDOM_NOISE_MEAN = 0\n",
    "    _RANDOM_NOISE_STANDARD_DEVIATION = 0.29578\n",
    "    \n",
    "    try:\n",
    "        quadratic_function_ouput = quadratic_function(x_value, _PARAMETER_A, _PARAMETER_B, _PARAMETER_C)\n",
    "        random_noise_contribution = sample_normal_distribution(_RANDOM_NOISE_MEAN, _RANDOM_NOISE_STANDARD_DEVIATION)\n",
    "        return quadratic_function_ouput + random_noise_contribution\n",
    "    \n",
    "    except Exception as E:\n",
    "        print(f\"> Error in the quadratic formula: {E}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we test the above code to see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35.876490793267116"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_underlying_function(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.4): Generating Pseudodata:\n",
    "\n",
    "Remember that ANNs need to train on data. Since we have no actual data, we need to generate our own; This reason is partially why we call it *pseudo*data. We generate pseudodata by running a huge loop that implements the sampling functions. What we want to obtain is a list of $x-$values and $y-$values such that they row-by-row correspond with one another. In other words, the $i-$th element in the $x-$value list ought to physically mean that our fancy experiment recorded that $x-$value and then measured the corresponding $i-$th element in the $y-$value list. More explicitly and less full of jargon, we're just saying that if we have the following two arrays,\n",
    "\n",
    "```python\n",
    "x_values = [1, 2, 3, ...]\n",
    "y_values = [5.4, 6.5, 7.5, ...]\n",
    "```\n",
    "\n",
    "then we are to interpret that our experiment measured $x$ as $2$ and then measured $y$ as $6.5$. These lists constitute our pseudodata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pseudodata(n_rounds):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    We generate, for N-rounds, a collection of outputs based on inputs.\n",
    "    In essence, this is just plugging-and-chugging, for a given list of \n",
    "    x-values, what some f(x) is. In this case, we are using the quadratic\n",
    "    function as the \"underlying function\" to generate our output.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "\n",
    "    Function Flow\n",
    "    --------------\n",
    "    \n",
    "    Notes\n",
    "    --------------\n",
    "    (1): \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    _are_sampling_normal_distribution = False\n",
    "    _X_MEAN_VALUE = 0\n",
    "    _X_STANDARD_DEVIATION = np.random.choice([1.2, 1.5], 1)[0]\n",
    "\n",
    "    pseudodata_x_values = []\n",
    "    pseudodata_y_values = []\n",
    "\n",
    "    for iteration in range(n_rounds):\n",
    "\n",
    "        if _are_sampling_normal_distribution:\n",
    "            randomly_sampled_x = sample_normal_distribution(_X_MEAN_VALUE, _X_STANDARD_DEVIATION)\n",
    "            \n",
    "        else:\n",
    "            randomly_sampled_x = 0.1 * np.random.randint(-100, 100)\n",
    "        pseudodata_x_values.append(randomly_sampled_x)\n",
    "        pseudodata_y_values.append(sample_underlying_function(randomly_sampled_x))\n",
    "\n",
    "    return pseudodata_x_values, pseudodata_y_values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we test to see if the function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.001000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n",
      "> Function \"quadratic_function\" took 0.000000 seconds to run.\n",
      "> Function \"sample_normal_distribution\" took 0.000000 seconds to run.\n",
      "> Function \"sample_underlying_function\" took 0.000000 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "X, Y = generate_pseudodata(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.5): Visualizing Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly code up a plotting function that just gives us a visual idea of how our pseudodata are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot_of_experimental_data(x_data, y_data):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Produce a plot of the experimental data: y vs. x. We are absolutely\n",
    "    not doing anything fancy here in the slightest regarding the aesthetics.\n",
    "    We'll do that later.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    x_data : Array-like:\n",
    "        The data along the x-axis.\n",
    "\n",
    "    y_data : Array-like\n",
    "        The data along the y-axis.\n",
    "\n",
    "        \n",
    "    Notes\n",
    "    --------------\n",
    "    This function replies on Matplotlib's Pyplot feature.\n",
    "\n",
    "    \"\"\"\n",
    "    return plt.scatter(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x229ed0ff4c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_plot_of_experimental_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5): Coding the Build-Your-Own-ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we turn our attention to the construction of an ANN that allows us to extract the underlying form of this function. The framework we are going to code our ANN in is Keras (TensorFlow). So, we need to first understand what the hell TensorFlow requires in order to actually set up and run a neural network. Then, following those requirements, we'll construct the functions that define the main things that constitute a neural network. Let's actually recap that right now.\n",
    "\n",
    "First of all, we're going to code up a bunch of boring prompt statements that should not vary in the code execution. This might not be the best practice, but it's fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PROMPT_NUMBER_1_NUMBER_OF_ANN_LAYERS = \"> How many layers do you want in your ANN architecture?\"\n",
    "INPUT_PROMPT_NUMBER_2_NODES_PER_LAYER = \"> Choose how many nodes you want for layer {}.\"\n",
    "INPUT_PROMPT_NUMBER_3_ACTIVATION_FUNCTION = \"> Choose an activation function for layer {}.\"\n",
    "INPUT_PROMPT_NUMBER_4_ACTIVATION_FUNCTION = \"> Please select the network's loss function.\"\n",
    "INPUT_PROMPT_NUMBER_5_INPUT_SHAPE = \"> Please specify the number of inputs of your network.\"\n",
    "\n",
    "ERROR_PROMPT_NUMBER_1_LAYER_NUMBER_NOT_INT = \"> Layer number must be a positive, nonzero integer. Continuing...\"\n",
    "ERROR_PROMPT_NUMBER_2_NODES_PER_LAYER_NOT_INTEGER = \"> Number of nodes per layer must be an integer type. Continuing...\"\n",
    "ERROR_PROMPT_NUMBER_3_ACTIVATION_FUNCTION_NOT_STRING = \"> Activation functions are of string datatype. Continuing...\"\n",
    "ERROR_PROMPT_NUMBER_3_ACTIVATION_FUNCTION_NOT_IN_LIST = \"> User input of `{}` is not included in known list of activation functions.\"\n",
    "ERROR_PROMPT_NUMBER_4_LOSS_FUNCTION_NOT_STRING = \"> The provided loss function was not a string type. Provide one that is. Continuing...\"\n",
    "ERROR_PROMPT_NUMBER_4_LOSS_FUNCTION_NOT_IN_LIST = \"> User input of `{}` is not included in known list of loss functions.\"\n",
    "ERROR_PROMPT_NUMBER_5_INPUT_SHAPE_INTEGER = \"> The user input of {} wasn't an integer.\"\n",
    "ERROR_PROMPT_NUMBER_5_INPUT_SHAPE_NEGATIVE = \"> Obviously, the size of the input vector cannot be zero or negative.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5.1): Obtain the Number of Hidden Layers in the Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_number_of_ANN_hidden_layers(verbose = False):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Obtain the number of hidden layers for the ANN from the user.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    verbose (boolean):\n",
    "        Do you want to see all output of this function evaluation?\n",
    "        \n",
    "    Notes\n",
    "    --------------\n",
    "    (1) We just need to obtain a nonzero, positive integer that\n",
    "    represents the number of layers in the ANN.\n",
    "\n",
    "    (2) https://stackoverflow.com/a/23294659 -> For a healthy way\n",
    "    to construct a while loop like this.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "\n",
    "        try:\n",
    "            \n",
    "            number_of_ANN_layers = int(input(INPUT_PROMPT_NUMBER_1_NUMBER_OF_ANN_LAYERS))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"> Received input: {number_of_ANN_layers} ({type(number_of_ANN_layers)}).\")\n",
    "\n",
    "        except ValueError:\n",
    "\n",
    "            print(ERROR_PROMPT_NUMBER_1_LAYER_NUMBER_NOT_INT)\n",
    "            continue\n",
    "\n",
    "        if number_of_ANN_layers <= 0:\n",
    "\n",
    "            print(ERROR_PROMPT_NUMBER_1_LAYER_NUMBER_NOT_INT)\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print(f\"> Received input: {number_of_ANN_layers} ({type(number_of_ANN_layers)}). Exiting...\")\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"> User defined {number_of_ANN_layers} Layers in the network.\")\n",
    "\n",
    "    return number_of_ANN_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Layer number must be a positive, nonzero integer. Continuing...\n",
      "> Received input: 3 (<class 'int'>). Exiting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtain_number_of_ANN_hidden_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5.2): For Each Layer, Obtain the Number of Nodes per Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_nodes_per_layer(number_of_ANN_layers, verbose = False):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Obtain the number of nodes per layer in the ANN.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    number_of_ANN_layers (int):\n",
    "        the number of layers in the ANN\n",
    "\n",
    "    verbose (boolean):\n",
    "        prints the intermediate computations of the function\n",
    "     \n",
    "    Notes\n",
    "    --------------\n",
    "    (1) For all each layer, we need to populate it with a number of neurons.\n",
    "        So, this function is about obtaining a list of intergers that correspond\n",
    "        to the number of neurons per layer.\n",
    "\n",
    "    (2) The output is a list of integers.\n",
    "    \"\"\"\n",
    "    list_of_number_of_nodes_per_layer = []\n",
    "\n",
    "    for layer_index in range(number_of_ANN_layers):\n",
    "\n",
    "        while True:\n",
    "\n",
    "            try:\n",
    "\n",
    "                number_of_nodes_per_given_layer = int(input(INPUT_PROMPT_NUMBER_2_NODES_PER_LAYER.format(layer_index + 1)))\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"> Received input: {number_of_nodes_per_given_layer} ({type(number_of_nodes_per_given_layer)}).\")\n",
    "\n",
    "            except ValueError:\n",
    "\n",
    "                print(ERROR_PROMPT_NUMBER_2_NODES_PER_LAYER_NOT_INTEGER)\n",
    "                continue\n",
    "\n",
    "            print(f\"> Received input: {number_of_nodes_per_given_layer} ({type(number_of_nodes_per_given_layer)}). Exiting...\")\n",
    "            list_of_number_of_nodes_per_layer.append(number_of_nodes_per_given_layer)\n",
    "            break\n",
    "        \n",
    "        print(f\"> User defined Layer #{layer_index + 1} to have {number_of_nodes_per_given_layer} nodes.\")\n",
    "\n",
    "    return list_of_number_of_nodes_per_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Received input: 5 (<class 'int'>). Exiting...\n",
      "> User defined Layer #1 to have 5 nodes.\n",
      "> Received input: 5 (<class 'int'>). Exiting...\n",
      "> User defined Layer #2 to have 5 nodes.\n",
      "> Received input: 1 (<class 'int'>). Exiting...\n",
      "> User defined Layer #3 to have 1 nodes.\n",
      "> Received input: 1 (<class 'int'>). Exiting...\n",
      "> User defined Layer #4 to have 1 nodes.\n",
      "> Received input: 1 (<class 'int'>). Exiting...\n",
      "> User defined Layer #5 to have 1 nodes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5, 5, 1, 1, 1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtain_nodes_per_layer(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5.3): For Each Layer, Obtain the Activation Function that will Apply to Each Node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_activation_function_included(user_entered_activation_function = None):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Sanitize/verify that the user-typed string corresponds to a given and known\n",
    "    activation function. Checks against the TensorFlow nomenclature.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    user_entered_activation_function (string):\n",
    "        the name of the desired activation function for the layer of nodes\n",
    "        \n",
    "    \n",
    "    Notes\n",
    "    --------------\n",
    "    (1): At this stage, we are actually validating the list of valid activation function\n",
    "        names. There's no real reason to do this, but it keeps us accountable.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _ACTIVATION_FUNCTIONS_STRING_ELU = \"elu\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_EXPONENTIAL = \"exponential\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_GELU = \"gelu\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_HARD_SIGMOID = \"hard_sigmoid\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_LINEAR = \"linear\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_MISH = \"mish\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_RELU = \"relu\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_SELU = \"selu\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_SIGMOID = \"sigmoid\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_SOFTMAX = \"softmax\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_SOFTPLUS = \"softplus\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_SOFTSIGN = \"softsign\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_SWISH= \"swish\"\n",
    "    _ACTIVATION_FUNCTIONS_STRING_TANH = \"tanh\"\n",
    "\n",
    "    _ARRAY_OF_ACCEPTABLE_ACTIVATION_FUNCTIONS = [\n",
    "        _ACTIVATION_FUNCTIONS_STRING_ELU,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_EXPONENTIAL,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_GELU,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_HARD_SIGMOID,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_LINEAR,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_MISH,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_RELU,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_SELU,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_SIGMOID,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_SOFTMAX,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_SOFTPLUS,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_SOFTSIGN,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_SWISH,\n",
    "        _ACTIVATION_FUNCTIONS_STRING_TANH,\n",
    "    ]\n",
    "    if user_entered_activation_function == None:\n",
    "        return False\n",
    "    else:\n",
    "        if user_entered_activation_function in _ARRAY_OF_ACCEPTABLE_ACTIVATION_FUNCTIONS:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_activation_function_included(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_activation_function_included(\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5.4): Actually Obtain the Activation Functions per Layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we need to do is construct a list of strings that will correspond to the activation functions that will apply to the nodes in a given layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_array_of_layer_activation_functions(number_of_ANN_layers, verbose = False):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Obtain the activation functions for each neuron in a given layer \n",
    "    from the user. There are only a few acceptable options for the \n",
    "    types of activation functions.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    number_of_ANN_layers (int): \n",
    "        The number of layers, not including the input data, in the network.\n",
    "    \n",
    "        \n",
    "    Notes\n",
    "    --------------\n",
    "    (1) For all neurons in a given layer, we will apply the same activation function.\n",
    "    \"\"\"\n",
    "    list_of_activation_functions_for_each_layer = []\n",
    "\n",
    "    for layer_index in range(number_of_ANN_layers):\n",
    "\n",
    "        while True:\n",
    "\n",
    "            try:\n",
    "\n",
    "                user_proposed_activation_function = str(input(INPUT_PROMPT_NUMBER_3_ACTIVATION_FUNCTION.format(layer_index + 1)))\n",
    "                if verbose:\n",
    "                    print(f\"> Received input: {user_proposed_activation_function} ({type(user_proposed_activation_function)}).\")\n",
    "\n",
    "            except ValueError:\n",
    "\n",
    "                print(ERROR_PROMPT_NUMBER_3_ACTIVATION_FUNCTION_NOT_STRING)\n",
    "                continue\n",
    "\n",
    "            if not check_if_activation_function_included(user_proposed_activation_function):\n",
    "\n",
    "                print(ERROR_PROMPT_NUMBER_3_ACTIVATION_FUNCTION_NOT_IN_LIST.format(user_proposed_activation_function))\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f\"> Received input: {user_proposed_activation_function} ({type(user_proposed_activation_function)}). Exiting...\")\n",
    "                list_of_activation_functions_for_each_layer.append(user_proposed_activation_function)\n",
    "                break\n",
    "        \n",
    "        print(f\"> User defined layer number {layer_index + 1} to use {user_proposed_activation_function} activation.\")\n",
    "\n",
    "    return list_of_activation_functions_for_each_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Received input: relu (<class 'str'>). Exiting...\n",
      "> User defined layer number 1 to use relu activation.\n",
      "> Received input: relu (<class 'str'>). Exiting...\n",
      "> User defined layer number 2 to use relu activation.\n",
      "> Received input: relu (<class 'str'>). Exiting...\n",
      "> User defined layer number 3 to use relu activation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['relu', 'relu', 'relu']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_array_of_layer_activation_functions(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5.5) Obtain the Model's Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do is provide an interface for the user to enter the name of a loss function that the network will use to perform backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_loss_function_included(user_entered_loss_function = None):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Sanitize/verify that the user-typed string corresponds to a given and known\n",
    "    activation function. Checks against the TensorFlow nomenclature.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    user_entered_activation_function (string): \n",
    "        the name of the desired loss functions\n",
    "        \n",
    "    \n",
    "    Notes\n",
    "    --------------\n",
    "    (1): This is just an intermediate validation. We want to make sure we\n",
    "        are referring to things that exist. That's why we put this \n",
    "        intermediate validation here.\n",
    "\n",
    "    \"\"\"\n",
    "    _ARRAY_OF_ACCEPTABLE_LOSS_FUNCTIONS = [\n",
    "        \"binary crossentropy\",\n",
    "        \"binary_focal crossentropy\",\n",
    "        \"categorical crossentropy\",\n",
    "        \"categorical focal crossentropy\",\n",
    "        \"categorical hinge\",\n",
    "        \"cosine similarity\",\n",
    "        \"hinge\",\n",
    "        \"huber\",\n",
    "        \"kl divergence\",\n",
    "        \"log cosh\",\n",
    "        \"loss\",\n",
    "        \"mean absolute error\",\n",
    "        \"mean absolute percentage error\",\n",
    "        \"mean squared error\",\n",
    "        \"mean squared logarithmic error\",\n",
    "        \"poisson\",\n",
    "        \"reduction\",\n",
    "        \"sparce categorical crossentropy\",\n",
    "        \"squared hinge\"\n",
    "    ]\n",
    "\n",
    "    if user_entered_loss_function == None:\n",
    "        return False\n",
    "    else:\n",
    "        if user_entered_loss_function in _ARRAY_OF_ACCEPTABLE_LOSS_FUNCTIONS:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "def choose_network_loss_function(verbose = False):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    We need a loss function in comparing the model's terrible output with\n",
    "    actual data. All this function involves is choosing the loss function.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "        verbose (boolean):\n",
    "            prints out the intermediate steps in a calculation\n",
    "    \n",
    "        \n",
    "    Notes\n",
    "    --------------\n",
    "    \"\"\"\n",
    "    while True:\n",
    "\n",
    "        try:\n",
    "\n",
    "            user_proposed_loss_function = str(input(INPUT_PROMPT_NUMBER_4_ACTIVATION_FUNCTION))\n",
    "            if verbose:\n",
    "                print(f\"> Received user input of {user_proposed_loss_function} ({type(user_proposed_loss_function)}).\")\n",
    "\n",
    "        except ValueError:\n",
    "\n",
    "            print(ERROR_PROMPT_NUMBER_4_LOSS_FUNCTION_NOT_STRING)\n",
    "            continue\n",
    "\n",
    "        if not check_if_loss_function_included(user_proposed_loss_function):\n",
    "\n",
    "            print(ERROR_PROMPT_NUMBER_4_LOSS_FUNCTION_NOT_IN_LIST.format(user_proposed_loss_function))\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"> Received user input of {user_proposed_loss_function} ({type(user_proposed_loss_function)}). Exiting...\")\n",
    "            break\n",
    "\n",
    "    return user_proposed_loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User input of `relu` is not included in known list of loss functions.\n",
      "> User input of `relu` is not included in known list of loss functions.\n",
      "> Received user input of mean squared error (<class 'str'>). Exiting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mean squared error'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_network_loss_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used ChatGPT with the following query to construct the next part of the code:\n",
    "\n",
    "> Write me a quick function in Python that accepts a user input. The user input should be a string, and I'd like it to be the name of a standard loss function. (You don't have to code any validation for this, but this is the general direction that we're going.) Once you have the string for the user's loss function, you need to then use that string to reference some sort of dictionary or other conveinent Python data type and return the corresponding Keras loss function instance. In other words, if my input is `\"mean squared\"`, then you need to return to me the actual instance of `tf.keras.losses.MeanSquaredError()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sent me the following response:\n",
    "\n",
    "> Certainly! You can create a function that takes a user input string, maps it to the corresponding Keras loss function instance using a dictionary, and returns that instance. Here's a simple example:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_loss_function(user_input):\n",
    "    # Define a dictionary mapping user inputs to Keras loss function instances\n",
    "    loss_function_mapping = {\n",
    "        \"mean squared\": tf.keras.losses.MeanSquaredError(),\n",
    "        \"mean absolute\": tf.keras.losses.MeanAbsoluteError(),\n",
    "        \"categorical crossentropy\": tf.keras.losses.CategoricalCrossentropy(),\n",
    "        # Add more loss functions as needed\n",
    "    }\n",
    "\n",
    "    # Convert user input to lowercase for case-insensitivity\n",
    "    user_input_lower = user_input.lower()\n",
    "\n",
    "    # Return the corresponding Keras loss function instance\n",
    "    return loss_function_mapping.get(user_input_lower, None)\n",
    "\n",
    "# Example usage\n",
    "user_loss_input = input(\"Enter a loss function name: \")\n",
    "loss_function_instance = get_loss_function(user_loss_input)\n",
    "\n",
    "if loss_function_instance:\n",
    "    print(f\"Selected Loss Function Instance: {loss_function_instance}\")\n",
    "else:\n",
    "    print(\"Invalid loss function name. Please enter a valid loss function.\")\n",
    "```\n",
    "\n",
    "> In this example, the get_loss_function function takes the user input, converts it to lowercase for case-insensitivity, and looks up the corresponding loss function instance in the loss_function_mapping dictionary. If the user input matches a known loss function, it returns the instance; otherwise, it returns None.\n",
    "\n",
    "> You can expand the loss_function_mapping dictionary with additional loss functions as needed for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just wanted to confirm that a legitimate way of doing the \"translation\" from a user input to some other thing that comes from a library can be implemented with a dictionary. I still don't know if this is common practice, but it does the job right now. So, I'm going to code up a dictionary to contains this correspondence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LOSSES_STRING_BINARY_CROSSENTROPY = \"binary crossentropy\"\n",
    "_LOSSES_STRING_BINARY_FOCAL_CROSSENTROPY = \"binary focal crossentropy\"\n",
    "_LOSSES_STRING_CATEGORICAL_CROSSENTROPY = \"categorical crossentropy\"\n",
    "_LOSSES_STRING_CATEGORICAL_FOCAL_CROSSENTROPY = \"categorical focal crossentropy\"\n",
    "_LOSSES_STRING_CATEGORICAL_HINGE = \"cateogrical hinge\"\n",
    "_LOSSES_STRING_COSINE_SIMILARITY = \"cosine similarity\"\n",
    "_LOSSES_STRING_HINGE = \"hinge\"\n",
    "_LOSSES_STRING_HUBER = \"huber\"\n",
    "_LOSSES_STRING_KL_DIVERGENCE = \"kl divergence\"\n",
    "_LOSSES_STRING_LOG_COSH = \"log cosh\"\n",
    "_LOSSES_STRING_LOSS = \"loss\"\n",
    "_LOSSES_STRING_MEAN_ABSOLUTE_ERROR = \"mean absolute error\"\n",
    "_LOSSES_STRING_MEAN_ABSOLULTE_PERCENTAGE_ERROR = \"mean absolute percentage error\"\n",
    "_LOSSES_STRING_MEAN_SQUARED_ERROR = \"mean squared error\"\n",
    "_LOSSES_STRING_MEAN_SQUARED_LOGARITHMIC_ERROR = \"mean squared logarithmic error\"\n",
    "_LOSSES_STRING_POISSON = \"poisson\"\n",
    "_LOSSES_STRING_REDUCTION = \"reduction\"\n",
    "_LOSSES_STRING_SPARCE_CATEGORICAL_CROSSENTROPY = \"sparce categorical crossentropy\"\n",
    "_LOSSES_STRING_SQUARED_HINGE = \"squared hinge\"\n",
    "\n",
    "_DICTIONARY_MAP_USER_INPUT_TO_KERAS_LOSS = {\n",
    "    _LOSSES_STRING_BINARY_CROSSENTROPY: tensorflow.keras.losses.BinaryCrossentropy(),\n",
    "    _LOSSES_STRING_BINARY_FOCAL_CROSSENTROPY: tensorflow.keras.losses.BinaryFocalCrossentropy(),\n",
    "    _LOSSES_STRING_CATEGORICAL_CROSSENTROPY: tensorflow.keras.losses.CategoricalCrossentropy(),\n",
    "    _LOSSES_STRING_CATEGORICAL_FOCAL_CROSSENTROPY: tensorflow.keras.losses.CategoricalFocalCrossentropy(),\n",
    "    _LOSSES_STRING_CATEGORICAL_HINGE: tensorflow.keras.losses.CategoricalHinge(),\n",
    "    _LOSSES_STRING_COSINE_SIMILARITY: tensorflow.keras.losses.CosineSimilarity(),\n",
    "    _LOSSES_STRING_HINGE: tensorflow.keras.losses.Hinge(),\n",
    "    _LOSSES_STRING_HUBER: tensorflow.keras.losses.Huber(),\n",
    "    _LOSSES_STRING_KL_DIVERGENCE: tensorflow.keras.losses.KLDivergence(),\n",
    "    _LOSSES_STRING_LOG_COSH: tensorflow.keras.losses.LogCosh(),\n",
    "    _LOSSES_STRING_LOSS: tensorflow.keras.losses.Loss(),\n",
    "    _LOSSES_STRING_MEAN_ABSOLUTE_ERROR: tensorflow.keras.losses.MeanAbsoluteError(),\n",
    "    _LOSSES_STRING_MEAN_ABSOLULTE_PERCENTAGE_ERROR: tensorflow.keras.losses.MeanAbsolutePercentageError(),\n",
    "    _LOSSES_STRING_MEAN_SQUARED_ERROR: tensorflow.keras.losses.MeanSquaredError(),\n",
    "    _LOSSES_STRING_MEAN_SQUARED_LOGARITHMIC_ERROR: tensorflow.keras.losses.MeanSquaredLogarithmicError(),\n",
    "    _LOSSES_STRING_POISSON: tensorflow.keras.losses.Poisson(),\n",
    "    _LOSSES_STRING_REDUCTION: tensorflow.keras.losses.Reduction(),\n",
    "    _LOSSES_STRING_SPARCE_CATEGORICAL_CROSSENTROPY: tensorflow.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    _LOSSES_STRING_SQUARED_HINGE: tensorflow.keras.losses.SquaredHinge(),\n",
    "}\n",
    "\n",
    "def translate_user_input_loss_function_to_keras_loss_function(string_user_input_loss_function):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    All this function does is it takes a string of user input that is supposed to\n",
    "    represent the name of some popular loss function (those included in TensorFlow)\n",
    "    and attempt to match it with the existing classes available. If the class\n",
    "    exists, we return that TF instance. If the class does not, then we tell the user\n",
    "    to get good.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    string_user_input_loss_function (string)\n",
    "        the name of the desired loss function as a string\n",
    "    \n",
    "        \n",
    "    Notes\n",
    "    --------------\n",
    "    (1): We reference a huge dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return _DICTIONARY_MAP_USER_INPUT_TO_KERAS_LOSS.get(string_user_input_loss_function, None)\n",
    "    \n",
    "    except Exception as E:\n",
    "        print(f\"> Error in translating input: {string_user_input_loss_function} to Keras Loss class instance:\\n> {E}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.losses.Loss at 0x229ef334130>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_user_input_loss_function_to_keras_loss_function(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.losses.MeanSquaredError at 0x229eceaba30>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_user_input_loss_function_to_keras_loss_function(\"mean squared error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to do the trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5.6): Obtaining/Creating the Model's Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create the `Input()` layer for our model. This isn't too difficult to do at all, but I wanted to turn this into a specific function anyway. The idea is that I am looking for the \"input dimension\" of the input layer. If you want to feed the network 10 different numbers, then the input dimension is 10. There are more advanced cases which I don't understand, like feeding 5 $N \\times N$ matrices as input to the network, but you should always be able to vectorize any kind of tensorial data structure. (That is my assumption.) So, let's code this easy function. It should take in an integer, which will come to represent the number of nodes in the first layer, and will simply return that number and pass it into an instance of the Keras `Input()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_number_of_input_variables(verbose = True):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    The number of input variables in the feed-forward network.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    number_of_input_variables (int): \n",
    "        The number of input variables we want to feed into the network.\n",
    "    \n",
    "        \n",
    "    Notes\n",
    "    --------------\n",
    "    (1) Unusually, the input layer is defined according to a \"vector\" of\n",
    "        length number_of_input_variables. So, we need to obtain the dimension\n",
    "        of this \"input vector.\" That's what this function is about.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "\n",
    "            try:\n",
    "\n",
    "                number_of_input_variables = int(input(INPUT_PROMPT_NUMBER_5_INPUT_SHAPE))\n",
    "                if verbose:\n",
    "                    print(f\"> Received input: {number_of_input_variables} ({type(number_of_input_variables)}).\")\n",
    "\n",
    "            except ValueError:\n",
    "\n",
    "                print(ERROR_PROMPT_NUMBER_5_INPUT_SHAPE_INTEGER.format(number_of_input_variables))\n",
    "                continue\n",
    "\n",
    "            if number_of_input_variables <= 0:\n",
    "\n",
    "                print(ERROR_PROMPT_NUMBER_5_INPUT_SHAPE_NEGATIVE)\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f\"> Received user input of {number_of_input_variables} ({type(number_of_input_variables)}). Exiting...\")\n",
    "                break\n",
    "        \n",
    "    return number_of_input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Received input: 1 (<class 'int'>).\n",
      "> Received user input of 1 (<class 'int'>). Exiting...\n",
      "WARNING:tensorflow:From C:\\Users\\fiore\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1 = obtain_number_of_input_variables()\n",
    "tensorflow.keras.layers.Input(shape = (test_1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5.7): Building the Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to actually build the model. In order to do so, the Keras API requires the kwargs `inputs` and `outputs`. We already know what our `inputs` will be, but we need to do something fancy to obtain the `outputs`. What we need to do is essentially stack recursive `Dense()` layers. This is not at all a fun exercise. \n",
    "\n",
    "As a quick example, suppose I have a single input layer and two hidden layers, each with 10 nodes. Then, I essentially need to write the following code:\n",
    "\n",
    "```python\n",
    "input_layer = tensorflow.keras.layers.Input(shape=(5))\n",
    "hidden_layer_1 = tensorflow.keras.layers.Dense(10)(input_layer)\n",
    "hidden_layer_2 = tensorflow.keras.layers.Dense(10)(hidden_layer_1)\n",
    "```\n",
    "\n",
    "From the above code, we would identify, when building the model, that `inputs = input_layer` and `outputs = hidden_layer_2`. Notice how there's some recursion going on. In order to obtain `hidden_layer_2`, we needed to pass in `hidden_layer_1` as its argument. In order to obtain `hidden_layer_1`, we needed to pass in `input_layer`. Because the goal of our code is to completely solve this issue of fixing the number of layers in a network and instead letting the user decide, we need to somehow turn this recursion based on a fixed number of layers and nodes per layers into one that is totally adaptive. Obviously, the key word here is \"recursion.\" Before we delve into that, let's just get a quick feel for what it is that we would need to code.\n",
    "\n",
    "If I expand out the code written above, i.e. do everything on a single line, then I would get something like this:\n",
    "\n",
    "```python\n",
    "hidden_layer_2 = tensorflow.keras.layers.Dense(10)(tensorflow.keras.layers.Dense(10)(tensorflow.keras.layers.Input(shape=(5))))\n",
    "```\n",
    "\n",
    "In other words, the general structure of our recursion must follow:\n",
    "\n",
    "```python\n",
    "output = Dense(nodes_in_layer_N)(layer_N_minus_1)\n",
    "       = Dense(nodes_in_layer_N)(Dense(nodes_in_layer_N_minus_1)(layer_N_minus_2))\n",
    "       = Dense(nodes_in_layer_N)(Dense(nodes_in_layer_N_minus_1)(Dense(nodes_in_layer_N_minus_2)(...)))\n",
    "```\n",
    "\n",
    "It is a challenge to figure out exactly how to do this recursion. In particular, we need to reference \"external data\" in the recursion, which is the number of nodes in layer $i$ and the activation functions used for that layer. What we'll do is prepare to reference those arrays with an index that decrements during the recursion. (This part of the code is a little difficult to explain right now, so see if you can understand what's happening just by scrolling down.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_output_layer_as_nested_hidden_layers(inputs, number_of_hidden_layers, list_of_nodes_per_layer, list_of_activation_functions, verbose = False):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    This function performs a recursive call to itself in order to properly\n",
    "    nest every instance of a Keras Dense() instance into an earlier one. That\n",
    "    is how we can stack Dense layers together.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    input (Keras Dense() or Input() instance): \n",
    "        the way this works is that we need to stack Dense()() layers\n",
    "        so that we can get the recursion Dense(N)(Dense(M)(Dense(P)(...))).\n",
    "\n",
    "    number_of_hidden_layers (int): \n",
    "        while we don't use a for-loop below, we are using this integer to \n",
    "        effectively perform a loop by decrementing an index number.\n",
    "    \n",
    "        \n",
    "    Notes\n",
    "    --------------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    if len(list_of_nodes_per_layer) is not len(list_of_activation_functions):\n",
    "        print(f\"> Mismatching length of lists: {len(list_of_nodes_per_layer)} in list of nodes per layer but {len(list_of_activation_functions)} in list of activation functions.\")\n",
    "        return None\n",
    "\n",
    "    if number_of_hidden_layers == 0:\n",
    "        return inputs\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        number_of_nodes_in_this_layer = list_of_nodes_per_layer[number_of_hidden_layers - 1]\n",
    "        activation_function_in_this_layer = list_of_activation_functions[number_of_hidden_layers - 1]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"> Now initializing a TF Dense() layer with {number_of_nodes_in_this_layer} nodes and {activation_function_in_this_layer} activations.\")\n",
    "            \n",
    "        nested_layer =  tensorflow.keras.layers.Dense(\n",
    "            number_of_nodes_in_this_layer,\n",
    "            activation = activation_function_in_this_layer,\n",
    "            use_bias = True,\n",
    "            kernel_initializer = 'glorot_uniform',\n",
    "            bias_initializer = 'zeros',\n",
    "            kernel_regularizer = None,\n",
    "            bias_regularizer = None,\n",
    "            activity_regularizer = None,\n",
    "            kernel_constraint = None,\n",
    "            bias_constraint = None,\n",
    "            )(calculate_output_layer_as_nested_hidden_layers(inputs, number_of_hidden_layers - 1, list_of_nodes_per_layer, list_of_activation_functions))\n",
    "        return nested_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try this function out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'dense')>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output_layer_as_nested_hidden_layers(\n",
    "    tensorflow.keras.layers.Input(shape =(3,)),\n",
    "    3,\n",
    "    [4, 5, 6],\n",
    "    [\"linear\", \"linear\", \"tanh\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that nesting feature actually worked, then we should be able to use `.summary()` on a TF Model. Let's verify that we see that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 16        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 25        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77 (308.00 Byte)\n",
      "Trainable params: 77 (308.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = tensorflow.keras.layers.Input(shape =(3,))\n",
    "output_of_network = calculate_output_layer_as_nested_hidden_layers(input_layer, 3, [4, 5, 6], [\"linear\", \"linear\", \"tanh\"])\n",
    "\n",
    "test_model = tensorflow.keras.Model(inputs=input_layer, outputs=output_of_network)\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5.8): Registering and Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't difficult at all. We're just going to write functions for it so that we can understand *when* the registration and compiling is taking place and zoom into those particular operations more closely in case we need to do fancier stuff later. What I call \"registering\" the model is merely a matter of instantiating `Model()` with those two important kwargs, `inputs` and `outputs`. Now that we've done the painstaking process of making functions that produce proper data for these kwargs, we simply need to code the function that passes them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_tensorflow_model(input_layer, output_layer):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    All this function does is return for us a Keras Model() instance\n",
    "    with the inputs and outputs kwargs specified. \n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    input_layer (Keras Input() instance): \n",
    "        the actual Keras Input() layer instance\n",
    "\n",
    "    output_layer (Keras Input() instance): \n",
    "        this should be some kind of Keras Layer() instance as \n",
    "        obtained by nesting layers together\n",
    "    \n",
    "        \n",
    "    Notes\n",
    "    --------------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        new_tensorflow_model = tensorflow.keras.models.Model(\n",
    "            inputs = input_layer,\n",
    "            outputs = output_layer\n",
    "        )\n",
    "        return new_tensorflow_model\n",
    "    \n",
    "    except Exception as E:\n",
    "        \n",
    "        print(f\"> Error in constructing a Keras model:\\n> {E}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_tensorflow_network(tensorflow_network, keras_loss_function):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Actually compile the TensorFlow ANN with all the details that \n",
    "    we supplied earlier. In reality, this function does nothing but\n",
    "    use the .compile() method, sets the (i) optimizer, (ii) loss,\n",
    "    and (iii) metrics.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    tensorflow_network (int): \n",
    "        The number of input variables we want to feed into the network.\n",
    "    \n",
    "        \n",
    "    Notes\n",
    "    --------------\n",
    "    (1) Unusually, the input layer is defined according to a \"vector\" of\n",
    "        length number_of_input_variables. So, we need to obtain the dimension\n",
    "        of this \"input vector.\" That's what this function is about.\n",
    "    \"\"\"\n",
    "    if tensorflow_network == None:\n",
    "        print(f\"> No TensorFlow network instance supplied. Exiting...\")\n",
    "        return None\n",
    "    \n",
    "    tensorflow_network.compile(\n",
    "        optimizer = tensorflow.keras.optimizers.Adam(), # tf.keras.optimizers\n",
    "        loss = keras_loss_function,\n",
    "        metrics = tensorflow.keras.metrics.MeanSquaredError(), # tf.keras.metrics\n",
    "        loss_weights = None,\n",
    "        weighted_metrics = None,\n",
    "        run_eagerly = None,\n",
    "        steps_per_execution = None,\n",
    "        jit_compile = None,\n",
    "        pss_evaluation_shards = 0,\n",
    "    )\n",
    "    return tensorflow_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (6): Testing the Entire Thing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to see if the entire thing works. We're going to run all the relevant functions in order and customize our own neural network. Upon seeing the results of this operation, we'll then actually *use* the network to attempt to fit some functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Received input: 3 (<class 'int'>). Exiting...\n",
      "> Received input: 1 (<class 'int'>). Exiting...\n",
      "> User defined Layer #1 to have 1 nodes.\n",
      "> Received input: 5 (<class 'int'>). Exiting...\n",
      "> User defined Layer #2 to have 5 nodes.\n",
      "> Received input: 1 (<class 'int'>). Exiting...\n",
      "> User defined Layer #3 to have 1 nodes.\n",
      "> Received input: relu (<class 'str'>). Exiting...\n",
      "> User defined layer number 1 to use relu activation.\n",
      "> Received input: relu (<class 'str'>). Exiting...\n",
      "> User defined layer number 2 to use relu activation.\n",
      "> Received input: relu (<class 'str'>). Exiting...\n",
      "> User defined layer number 3 to use relu activation.\n",
      "> Received user input of mean squared error (<class 'str'>). Exiting...\n",
      "> Received input: 1 (<class 'int'>).\n",
      "> Received user input of 1 (<class 'int'>). Exiting...\n"
     ]
    }
   ],
   "source": [
    "# (1): Obtain the number of ANN hidden layers:\n",
    "ann_hidden_layer_number = obtain_number_of_ANN_hidden_layers()\n",
    "\n",
    "# (2): Obtain the list for the number of nodes per layer:\n",
    "list_of_nodes_per_layer = obtain_nodes_per_layer(ann_hidden_layer_number)\n",
    "\n",
    "# (3): Obtain the name of the activation function for all nodes in each layer:\n",
    "list_of_activation_functions = construct_array_of_layer_activation_functions(ann_hidden_layer_number)\n",
    "\n",
    "# (4): Choose a network loss function:\n",
    "loss_function = choose_network_loss_function()\n",
    "keras_loss_function = translate_user_input_loss_function_to_keras_loss_function(loss_function)\n",
    "\n",
    "# (5): Obtain the number of input variables:\n",
    "number_of_input_variables = obtain_number_of_input_variables()\n",
    "inputs_to_network = tensorflow.keras.layers.Input(shape = (number_of_input_variables,))\n",
    "\n",
    "# (6): Generate the Keras output layer:\n",
    "outputs_of_network =  calculate_output_layer_as_nested_hidden_layers(inputs_to_network, ann_hidden_layer_number, list_of_nodes_per_layer, list_of_activation_functions)\n",
    "\n",
    "# (7): Register the Keras model:\n",
    "new_network = register_tensorflow_model(inputs_to_network, outputs_of_network)\n",
    "\n",
    "# (8): Compile the Network:\n",
    "compiled_new_network = compile_tensorflow_network(new_network, keras_loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 2         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 10        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18 (72.00 Byte)\n",
      "Trainable params: 18 (72.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "compiled_new_network.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
